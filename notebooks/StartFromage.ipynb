{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c3291-18ef-47e1-84ac-26cbaaabca4a",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "1. Only implemented image captioning setup, and now implementing image-text-retrieval part.\n",
    "3. Also, now only works with single token, we can learn multiple tokens later.\n",
    "4. Currently does not accept any prompts, will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2488486f-1fad-49b5-8fd0-094c2ce0d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/oince22/.conda/envs/fromage_scratch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Optional, Tuple, List, Dict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pydicom as dicom\n",
    "import os\n",
    "from torchvision.models import resnet50\n",
    "from glob import glob\n",
    "import random\n",
    "from transformers import OPTForCausalLM, AutoTokenizer, BioGptTokenizer, BioGptForCausalLM\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import csv\n",
    "import re\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077d8a80-a8fd-4df8-881d-1643781d87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioViL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50()\n",
    "        self._initialize_resnet()\n",
    "        self.feature_extractor = self._get_feature_extractor()\n",
    "        \n",
    "    def _initialize_resnet(self):\n",
    "        model_state_dict = torch.load(\"biovil_backbone_2048.pt\")\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    def _get_feature_extractor(self):\n",
    "        self._return_nodes = {'avgpool': 'avgpool'}\n",
    "        return create_feature_extractor(self.model, return_nodes=self._return_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)[\"avgpool\"]\n",
    "        features = features.squeeze()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc2b24d-2b73-4ac6-a626-405e0bc099f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_to_uint8(array: np.ndarray, percentiles: Optional[Tuple[float, float]] = None) -> np.ndarray:\n",
    "    \"\"\"Remap values in input so the output range is :math:`[0, 255]`.\n",
    "\n",
    "    Percentiles can be used to specify the range of values to remap.\n",
    "    This is useful to discard outliers in the input data.\n",
    "\n",
    "    :param array: Input array.\n",
    "    :param percentiles: Percentiles of the input values that will be mapped to ``0`` and ``255``.\n",
    "        Passing ``None`` is equivalent to using percentiles ``(0, 100)`` (but faster).\n",
    "    :returns: Array with ``0`` and ``255`` as minimum and maximum values.\n",
    "    \"\"\"\n",
    "    array = array.astype(float)\n",
    "    if percentiles is not None:\n",
    "        len_percentiles = len(percentiles)\n",
    "        if len_percentiles != 2:\n",
    "            message = 'The value for percentiles should be a sequence of length 2,' f' but has length {len_percentiles}'\n",
    "            raise ValueError(message)\n",
    "        a, b = percentiles\n",
    "        if a >= b:\n",
    "            raise ValueError(f'Percentiles must be in ascending order, but a sequence \"{percentiles}\" was passed')\n",
    "        if a < 0 or b > 100:\n",
    "            raise ValueError(f'Percentiles must be in the range [0, 100], but a sequence \"{percentiles}\" was passed')\n",
    "        cutoff: np.ndarray = np.percentile(array, percentiles)\n",
    "        array = np.clip(array, *cutoff)\n",
    "    array -= array.min()\n",
    "    array /= array.max()\n",
    "    array *= 255\n",
    "    return array.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96cb6a5c-3177-4183-9427-65aa02216ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: Path) -> Image.Image:\n",
    "    \"\"\"Load an image from disk.\n",
    "\n",
    "    The image values are remapped to :math:`[0, 255]` and cast to 8-bit unsigned integers.\n",
    "\n",
    "    :param path: Path to image.\n",
    "    :returns: Image as ``Pillow`` ``Image``.\n",
    "    \"\"\"\n",
    "    # Although ITK supports JPEG and PNG, we use Pillow for consistency with older trained models\n",
    "    if path.suffix in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        image = io.imread(path)\n",
    "    elif path.suffix == \".dcm\":\n",
    "        image = dicom.dcmread(path).pixel_array\n",
    "    else:\n",
    "        raise ValueError(f\"Image type not supported, filename was: {path}\")\n",
    "\n",
    "    image = remap_to_uint8(image)\n",
    "    return Image.fromarray(image).convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8999769-62ea-4867-a459-ef13dba0dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandChannels:\n",
    "    def __call__(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        if data.shape[0] != 1:\n",
    "            raise ValueError(f\"Expected input of shape [1, H, W], found {data.shape}\")\n",
    "        return torch.repeat_interleave(data, 3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b76bc1-00e7-453d-bacd-c2d6862f25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_report(text):\n",
    "    # Remove unnecessary and insensible parts\n",
    "    text = re.sub(r\"EXAMINATION:.*\", \"\", text)  # Remove EXAMINATION line\n",
    "    text = re.sub(r\"WET READ:.*\", \"\", text)  # Remove WET READ line\n",
    "    text = re.sub(r\"FINAL REPORT\", \"\", text)  # Remove FINAL REPORT line\n",
    "    text = re.sub(r\"STUDY:.*\", \"\", text)  # Remove STUDY line\n",
    "    text = re.sub(r\"COMPARISON:.*\", \"\", text)  # Remove COMPARISON section\n",
    "    text = re.sub(r\"TECHNIQUE:.*\", \"\", text)  # Remove TECHNIQUE section\n",
    "    text = re.sub(r\"_+\", \"_\", text)  # Remove multiple underscores\n",
    "\n",
    "    # Clean up excessive newlines and spaces\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd4789a-e33c-4356-822d-59241a63a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, CenterCrop, RandomHorizontalFlip, RandomAffine\n",
    "\n",
    "# values from BioViL repository\n",
    "RESIZE = 512\n",
    "CENTER_CROP_SIZE = 480\n",
    "\n",
    "def create_chest_xray_transform_for_inference(resize: int, center_crop_size: int, train: bool) -> Compose:\n",
    "    data_aug_rot = 15\n",
    "    data_aug_trans = 0.10\n",
    "    data_aug_scale = 0.10\n",
    "    if not train:\n",
    "        transforms = [Resize(resize), CenterCrop(center_crop_size), ToTensor(), ExpandChannels()]\n",
    "    if train:\n",
    "        transforms = [Resize(resize), \n",
    "                        RandomAffine(data_aug_rot, \n",
    "                            translate=(data_aug_trans, data_aug_trans), \n",
    "                            scale=(1.0-data_aug_scale, 1.0+data_aug_scale)\n",
    "                        ),\n",
    "                      CenterCrop(center_crop_size), ToTensor(), ExpandChannels()]\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1fccb1-bab8-400b-81eb-32ca192d818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(torch.utils.data.Dataset):\n",
    "    IMG_ROOT = '/kuacc/users/oince22/hpc_run/physionet.org/files/mimic-cxr-jpg/2.0.0/files/'\n",
    "    \n",
    "    def __init__(self, tsv_fname, transform):\n",
    "        self.tsv_fname = tsv_fname\n",
    "        self.img_paths, self.reports = self._read_tsv_file()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _read_tsv_file(self):\n",
    "        reports = []\n",
    "        img_paths = []\n",
    "        with open(self.tsv_fname, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for report, img_path in reader:\n",
    "                reports.append(report)\n",
    "                img_paths.append(Path(MIMICDataset.IMG_ROOT + img_path))\n",
    "                \n",
    "        return img_paths, reports\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            try:\n",
    "                img = load_image(self.img_paths[idx])\n",
    "                text = self.reports[idx]\n",
    "                transform_img = self.transform(img)\n",
    "                return transform_img, text\n",
    "            except:\n",
    "                idx = np.random.randint(0, len(self.img_paths))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ff243b-ec97-4224-86ba-849ffb524696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(model_checkpoint, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FromageModel.MODEL_CHECKPOINT, device=device)\n",
    "    tokenizer.add_special_tokens({\"cls_token\": \"<|image|>\"})\n",
    "    tokenizer.add_tokens(\"[RET]\")\n",
    "    ret_id = tokenizer('[RET]', add_special_tokens=False).input_ids\n",
    "    assert len(ret_id) == 1, \"Failed to add [RET] token to tokenizer\"\n",
    "    ret_token_idx = ret_id[0]\n",
    "    return tokenizer, ret_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65011b7e-a9eb-4683-bd3a-f16b42859398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(logits):\n",
    "    return F.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1f779b-4a16-4585-8cf3-3224d7469f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = create_chest_xray_transform_for_inference(RESIZE, CENTER_CROP_SIZE, train=True)\n",
    "dataset = MIMICDataset(\"MIMIC_JPG.tsv\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030feda4-38d8-4908-9b67-5efef08bd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=84, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ceb7631-148d-44df-8a0a-ea6338a816f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FromageModel(nn.Module):\n",
    "    MODEL_CHECKPOINT = \"facebook/opt-350m\"\n",
    "    VISION_EMBED_DIM = 2048\n",
    "    VISION_EMBED_DROPOUT = 0.1\n",
    "    SHARED_EMB_DIM = 512\n",
    "    \n",
    "    def __init__(self, device, tokenizer, ret_token_idx):\n",
    "        super().__init__()\n",
    "        self.ret_token_idx = ret_token_idx\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.image_token = self.tokenizer.cls_token_id\n",
    "        \n",
    "        self.lm = OPTForCausalLM.from_pretrained(FromageModel.MODEL_CHECKPOINT)\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.vm = BioViL()\n",
    "        for param in self.vm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lm.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.input_embeddings = self.lm.get_input_embeddings()\n",
    "        \n",
    "        self.LM_EMBED_DIM = self.input_embeddings.embedding_dim\n",
    "        \n",
    "        self.vm.eval()\n",
    "        self.lm.eval()\n",
    "        \n",
    "        # currently only works with one token, we will generalize it to multiple tokens later\n",
    "        self.caption_mapping = nn.Linear(FromageModel.VISION_EMBED_DIM, self.LM_EMBED_DIM)\n",
    "        self.mapping_dropout = nn.Dropout(FromageModel.VISION_EMBED_DROPOUT)\n",
    "\n",
    "        self.ret_i2t_mapping = nn.Linear(FromageModel.VISION_EMBED_DIM, FromageModel.SHARED_EMB_DIM)\n",
    "        self.ret_t2i_mapping = nn.Linear(self.LM_EMBED_DIM, FromageModel.SHARED_EMB_DIM)\n",
    "        \n",
    "    def generate(self, embeddings, max_len, temperature=0.0, top_p=1.0, filter_value=float(\"-inf\")):\n",
    "        bsz, seq_len, _ = embeddings.shape\n",
    "        out = None\n",
    "        past_key_values = None\n",
    "        output_embeddings = []\n",
    "        output_logits = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_len):\n",
    "                output = self.lm(inputs_embeds=embeddings, use_cache=False, output_hidden_states=True)\n",
    "                last_hidden_state = output.hidden_states[-1]\n",
    "                last_hidden_state = last_hidden_state[torch.arange(last_hidden_state.shape[0]), seq_len-1, :]\n",
    "                last_hidden_state = self.ret_t2i_mapping(last_hidden_state)\n",
    "                last_embedding = last_hidden_state / last_hidden_state.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "                output_embeddings.append(last_embedding)\n",
    "                logits = output.logits[:,-1,:] # todo, look at here again\n",
    "                output_logits.append(logits)\n",
    "    \n",
    "                if temperature == 0.0:\n",
    "                    if top_p != 1.0:\n",
    "                        assert False, \"top_p cannot be set in greedy decoding\"\n",
    "                    next_token = torch.argmax(logits, keepdim=True, dim=-1)\n",
    "                else:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "                if top_p < 1.0:\n",
    "                    assert top_p > 0, f\"0 < top_p <= 1\"\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                    for j in range(sorted_indices.shape[0]):\n",
    "                        indices_to_remove = sorted_indices[j, sorted_indices_to_remove[j, :]]\n",
    "                        logits[j, indices_to_remove] = filter_value\n",
    "            \n",
    "                    token_weights = logits.exp()\n",
    "                    next_token = torch.multinomial(token_weights, 1)\n",
    "\n",
    "                next_token = next_token.long().to(embeddings.device)\n",
    "                if out is not None:\n",
    "                    out = torch.cat([out, next_token], dim=1)\n",
    "                else:\n",
    "                    out = next_token\n",
    "\n",
    "\n",
    "                next_embedding = self.input_embeddings(next_token)\n",
    "                embeddings = torch.cat([embeddings, next_embedding], dim=1)\n",
    "\n",
    "                if (self.tokenizer.eos_token_id and (next_token == self.tokenizer.eos_token_id).all()):\n",
    "                    break\n",
    "\n",
    "        return out, output_embeddings, output_logits\n",
    "\n",
    "    def get_vis_embs(self, pixel_values, mode):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        img_embs = self.vm(pixel_values)\n",
    "\n",
    "        if mode == \"caption\":\n",
    "            img_embs = self.caption_mapping(img_embs)\n",
    "            img_embs = self.mapping_dropout(img_embs)\n",
    "        elif mode == \"retrieval\":\n",
    "            img_embs = self.ret_i2t_mapping(img_embs)\n",
    "            img_embs = self.mapping_dropout(img_embs)\n",
    "\n",
    "        return img_embs\n",
    "            \n",
    "    \n",
    "    def forward(self, pixel_values, text_inputs, mode):\n",
    "        assert mode in [\"caption\", \"retrieval\"], f'Mode must be in [\"caption\", \"retrieval\"], got {mode} instead'\n",
    "        \n",
    "        if mode == \"retrieval\":\n",
    "            new_text_inputs = []\n",
    "            for i in range(len(text_inputs)):\n",
    "                new_text_inputs.append(f'{text_inputs[i]}[RET]')\n",
    "            text_inputs = tuple(new_text_inputs)\n",
    "        \n",
    "        text_inputs = self.tokenizer(text_inputs, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=112).to(device)\n",
    "        text_lens = text_inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        if mode == \"retrieval\":\n",
    "            for idx in range(len(text_inputs.input_ids)):\n",
    "                if text_inputs.input_ids[idx][text_lens[idx]-1] != self.ret_token_idx:\n",
    "                    text_inputs.input_ids[idx][text_lens[idx]-1] = self.ret_token_idx\n",
    "\n",
    "        t2i_embs, i2t_embs = None, None\n",
    "\n",
    "        if mode == \"caption\":\n",
    "            img_embs = self.get_vis_embs(pixel_values, mode=mode)\n",
    "            img_embs = img_embs.unsqueeze(1)\n",
    "\n",
    "            labels = text_inputs.input_ids\n",
    "            text_embs = self.input_embeddings(labels)\n",
    "            additional_mask = torch.ones(img_embs.shape[:2], dtype=torch.int64).to(self.device)\n",
    "            attention_mask = torch.cat([additional_mask, text_inputs.attention_mask], dim=1)\n",
    "        \n",
    "            full_labels = torch.full(img_embs.shape[:2], -100).to(self.device)\n",
    "            full_labels = torch.cat([full_labels, labels], dim=1)\n",
    "            \n",
    "            input_embs = torch.cat([img_embs, text_embs], dim=1)\n",
    "        \n",
    "            output = self.lm(inputs_embeds=input_embs, attention_mask=attention_mask, labels=full_labels, output_hidden_states=True)\n",
    "        \n",
    "        elif mode == \"retrieval\":\n",
    "            i2t_embs = self.get_vis_embs(pixel_values, mode=mode)\n",
    "\n",
    "            labels = text_inputs.input_ids\n",
    "            text_embs = self.input_embeddings(labels)\n",
    "            input_embs = text_embs\n",
    "\n",
    "            output = self.lm(inputs_embeds=input_embs, attention_mask=text_inputs.attention_mask, labels=labels, output_hidden_states=True)\n",
    "\n",
    "            t2i_embs = output.hidden_states[-1]\n",
    "            t2i_embs = t2i_embs[torch.arange(t2i_embs.shape[0]), text_lens-1, :]\n",
    "            t2i_embs = self.ret_t2i_mapping(t2i_embs)\n",
    "\n",
    "            i2t_embs = i2t_embs / i2t_embs.norm(dim=1, keepdim=True)\n",
    "            t2i_embs = t2i_embs / t2i_embs.norm(dim=1, keepdim=True)\n",
    "\n",
    "            i2t_embs = self.logit_scale.exp() * i2t_embs\n",
    "        \n",
    "        return output, t2i_embs, i2t_embs\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super(FromageModel, self).train(mode=mode)\n",
    "        self.vm.eval()\n",
    "        self.lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f63aaf86-e067-4505-a54a-85c3d4112d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fromage(nn.Module):\n",
    "    def __init__(self, device, tokenizer, ret_token_idx, resize=512, center_crop_size=480):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.ret_token_idx = ret_token_idx\n",
    "        self.model = FromageModel(device=device, tokenizer=tokenizer, ret_token_idx=ret_token_idx)\n",
    "        self.img_transform = create_chest_xray_transform_for_inference(resize=resize, center_crop_size=center_crop_size, train=False)\n",
    "\n",
    "    def __call__(self, images, tgt_tokens=None, generate=False, max_len=96, temperature=0.0, top_p=1.0, mode=\"caption\", inference=False):\n",
    "        if generate:\n",
    "            return self.model.generate(embeddings=images, max_len=max_len, temperature=temperature, top_p=top_p)\n",
    "\n",
    "        return self.model(pixel_values=images, text_inputs=tgt_tokens, mode=mode)\n",
    "\n",
    "    def generate_for_images_and_texts(self, prompts: List, max_len=32, top_p=1.0, temperature=0.0):\n",
    "        input_embs = []\n",
    "        input_ids = []\n",
    "\n",
    "        add_bos = True\n",
    "\n",
    "        for i, p in enumerate(prompts):\n",
    "            if isinstance(p, Path):\n",
    "                img = load_image(p)\n",
    "                pixel_values = self.img_transform(img)\n",
    "                pixel_values = pixel_values[None, ...]\n",
    "                vis_emb = self.model.get_vis_embs(pixel_values, mode=\"caption\")\n",
    "                vis_emb = vis_emb.unsqueeze(0).unsqueeze(0)\n",
    "                input_embs.append(vis_emb)\n",
    "            elif type(p) == str:\n",
    "                tokens = self.model.tokenizer(p, add_special_tokens=True, return_tensors=\"pt\")\n",
    "                text_ids = tokens.input_ids.to(self.device)\n",
    "                if not add_bos:\n",
    "                    text_ids = text_ids[:, 1:]\n",
    "                else:\n",
    "                    add_bos = False\n",
    "\n",
    "                text_embs = self.model.input_embeddings(text_ids)\n",
    "                input_embs.append(text_embs)\n",
    "                input_ids.append(text_ids)\n",
    "            else:\n",
    "                assert False, \"Prompt type can only be Path for images or string for text\"\n",
    "\n",
    "        input_embs = torch.cat(input_embs, dim=1)\n",
    "        input_ids = torch.cat(input_ids, dim=1)\n",
    "\n",
    "        generated_ids, generated_embeddings, _ = self.model.generate(input_embs, max_len, temperature=temperature, top_p=top_p)\n",
    "\n",
    "        return_outputs = self.model.tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]        \n",
    "        return return_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fd058-51a2-48e7-8546-cfb0cfa2ad3a",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "**Why do we use is `torch.zeros - 100` for labels of input embeddings?**\n",
    "\n",
    "### Answer:\n",
    "\n",
    "One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in ðŸ¤— Transformers by setting the labels we wish to ignore to -100. In the example above, if the label for @HuggingFace is 3 (indexing B-corporation), we would set the labels of `['@', 'hugging', '##face']` to `[3, -100, -100]`.\n",
    "\n",
    "Source: [https://huggingface.co/transformers/v4.4.2/custom_datasets.html](https://huggingface.co/transformers/v4.4.2/custom_datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc75719-f960-4e49-bb6a-b591291dc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, ret_token_idx = setup_tokenizer(FromageModel.MODEL_CHECKPOINT, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb7dfa7-cd16-44f4-b265-310e4c4bd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fromage(device, tokenizer, ret_token_idx).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "EPOCH = 50\n",
    "MODES = (\"caption\", \"retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba74452-0283-4c85-b8ef-f0ceefcdeeea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoint = torch.load(\"best.ckpt\")\n",
    "model.load_state_dict(best_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "525667d7-3f6b-4702-b0db-303a216e11dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDICATION: History: F with epigastric pain, vomiting FINDINGS: Heart size is mildly enlarged with evidence of prior mitral valve replacement. Clips are seen projecting over the right hilum. Mediastinal and hilar contours are unchanged with mild atherosclerotic calcifications noted diffusely. Lungs are hyperinflated but grossly clear without focal consolidation, pleural effusion or pneumothorax. The osseous structures are diffusely demineralized. Bilateral shoulder prostheses are partially imaged. IMPRESSION: No acute cardiopulmonary abnormality.\n"
     ]
    }
   ],
   "source": [
    "ex_idx = random.randint(0, len(dataset.img_paths) - 1)\n",
    "ex_img_path, ex_report = dataset.img_paths[ex_idx], dataset.reports[ex_idx]\n",
    "print(ex_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0371a5a1-db2b-4755-8702-e40875cb8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iliac-pulmonary-ventricular (PVC) and ventricular (VVC) size is iliac-pulmonary-ventricular\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    prompts = [ex_img_path, \"INDICATION: History: F with epigastric pain, vomiting FINDINGS: Heart size is \"] # \" \".join(ex_report.split()[:5])\n",
    "    print(model.generate_for_images_and_texts(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988d10f-1c78-4fdc-a118-110899bbe0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1 Step 0 -- caption Loss: 2.076 retrieval Loss: 3.870 \n",
      "Current best loss!\n",
      "Epoch 1 Step 15 -- caption Loss: 2.075 retrieval Loss: 3.880 \n",
      "Current best loss!\n",
      "Epoch 1 Step 30 -- caption Loss: 2.067 retrieval Loss: 3.873 \n",
      "Current best loss!\n",
      "Epoch 1 Step 45 -- caption Loss: 2.064 retrieval Loss: 3.864 \n",
      "Current best loss!\n",
      "Epoch 1 Step 60 -- caption Loss: 2.060 retrieval Loss: 3.854 \n",
      "Current best loss!\n",
      "Epoch 1 Step 75 -- caption Loss: 2.060 retrieval Loss: 3.847 \n",
      "Current best loss!\n",
      "Epoch 1 Step 90 -- caption Loss: 2.059 retrieval Loss: 3.844 \n",
      "Current best loss!\n",
      "Epoch 1 Step 105 -- caption Loss: 2.061 retrieval Loss: 3.840 \n",
      "Current best loss!\n",
      "Epoch 1 Step 120 -- caption Loss: 2.060 retrieval Loss: 3.836 \n",
      "Current best loss!\n",
      "Epoch 1 Step 135 -- caption Loss: 2.058 retrieval Loss: 3.834 \n",
      "Current best loss!\n",
      "Epoch 1 Step 150 -- caption Loss: 2.058 retrieval Loss: 3.832 \n",
      "Current best loss!\n",
      "Epoch 1 Step 165 -- caption Loss: 2.057 retrieval Loss: 3.831 \n",
      "Current best loss!\n",
      "Epoch 1 Step 180 -- caption Loss: 2.058 retrieval Loss: 3.829 \n",
      "Current best loss!\n",
      "Epoch 1 Step 195 -- caption Loss: 2.058 retrieval Loss: 3.825 \n",
      "Current best loss!\n",
      "Epoch 1 Step 210 -- caption Loss: 2.058 retrieval Loss: 3.820 \n",
      "Current best loss!\n",
      "Epoch 1 Step 225 -- caption Loss: 2.057 retrieval Loss: 3.817 \n",
      "Current best loss!\n",
      "Epoch 1 Step 240 -- caption Loss: 2.056 retrieval Loss: 3.812 \n",
      "Current best loss!\n",
      "Epoch 1 Step 255 -- caption Loss: 2.056 retrieval Loss: 3.808 \n",
      "Current best loss!\n",
      "Epoch 1 Step 270 -- caption Loss: 2.056 retrieval Loss: 3.805 \n",
      "Current best loss!\n",
      "Epoch 1 Step 285 -- caption Loss: 2.056 retrieval Loss: 3.801 \n",
      "Current best loss!\n",
      "Epoch 1 Step 300 -- caption Loss: 2.055 retrieval Loss: 3.799 \n",
      "Current best loss!\n",
      "Epoch 1 Step 315 -- caption Loss: 2.056 retrieval Loss: 3.798 \n",
      "Current best loss!\n",
      "Epoch 1 Step 330 -- caption Loss: 2.056 retrieval Loss: 3.796 \n",
      "Current best loss!\n",
      "Epoch 1 Step 345 -- caption Loss: 2.055 retrieval Loss: 3.793 \n",
      "Current best loss!\n",
      "Epoch 1 Step 360 -- caption Loss: 2.055 retrieval Loss: 3.789 \n",
      "Current best loss!\n",
      "Epoch 1 Step 375 -- caption Loss: 2.055 retrieval Loss: 3.787 \n",
      "Current best loss!\n",
      "Epoch 1 Step 390 -- caption Loss: 2.055 retrieval Loss: 3.784 \n",
      "Current best loss!\n",
      "Epoch 1 Step 405 -- caption Loss: 2.054 retrieval Loss: 3.783 \n",
      "Current best loss!\n",
      "Epoch 1 Step 420 -- caption Loss: 2.054 retrieval Loss: 3.779 \n",
      "Current best loss!\n",
      "Epoch 1 Step 435 -- caption Loss: 2.053 retrieval Loss: 3.777 \n",
      "Current best loss!\n",
      "Epoch 1 Step 450 -- caption Loss: 2.053 retrieval Loss: 3.775 \n",
      "Current best loss!\n",
      "Epoch 1 Step 465 -- caption Loss: 2.052 retrieval Loss: 3.772 \n",
      "Current best loss!\n",
      "Epoch 1 Step 480 -- caption Loss: 2.052 retrieval Loss: 3.769 \n",
      "Current best loss!\n",
      "Epoch 1 Step 495 -- caption Loss: 2.052 retrieval Loss: 3.767 \n",
      "Current best loss!\n",
      "Epoch 1 Step 510 -- caption Loss: 2.052 retrieval Loss: 3.765 \n",
      "Current best loss!\n",
      "Epoch 1 Step 525 -- caption Loss: 2.052 retrieval Loss: 3.763 \n",
      "Current best loss!\n",
      "Epoch 1 Step 540 -- caption Loss: 2.052 retrieval Loss: 3.761 \n",
      "Current best loss!\n",
      "Epoch 1 Step 555 -- caption Loss: 2.052 retrieval Loss: 3.758 \n",
      "Current best loss!\n",
      "Epoch 1 Step 570 -- caption Loss: 2.052 retrieval Loss: 3.755 \n",
      "Current best loss!\n",
      "Epoch 1 Step 585 -- caption Loss: 2.052 retrieval Loss: 3.754 \n",
      "Current best loss!\n",
      "Epoch 1 Step 600 -- caption Loss: 2.052 retrieval Loss: 3.751 \n",
      "Current best loss!\n",
      "Epoch 1 Step 615 -- caption Loss: 2.052 retrieval Loss: 3.749 \n",
      "Current best loss!\n",
      "Epoch 1 Step 630 -- caption Loss: 2.051 retrieval Loss: 3.746 \n",
      "Current best loss!\n",
      "Epoch 1 Step 645 -- caption Loss: 2.051 retrieval Loss: 3.744 \n",
      "Current best loss!\n",
      "Epoch 1 Step 660 -- caption Loss: 2.051 retrieval Loss: 3.742 \n",
      "Current best loss!\n",
      "Epoch 1 Step 675 -- caption Loss: 2.051 retrieval Loss: 3.740 \n",
      "Current best loss!\n",
      "Epoch 1 Step 690 -- caption Loss: 2.051 retrieval Loss: 3.739 \n",
      "Current best loss!\n",
      "Epoch 1 Step 705 -- caption Loss: 2.051 retrieval Loss: 3.737 \n",
      "Current best loss!\n",
      "Epoch 1 Step 720 -- caption Loss: 2.051 retrieval Loss: 3.736 \n",
      "Current best loss!\n",
      "Epoch 1 Step 735 -- caption Loss: 2.051 retrieval Loss: 3.733 \n",
      "Current best loss!\n",
      "Epoch 1 Step 750 -- caption Loss: 2.051 retrieval Loss: 3.731 \n",
      "Current best loss!\n",
      "Epoch 1 Step 765 -- caption Loss: 2.051 retrieval Loss: 3.729 \n",
      "Current best loss!\n",
      "Epoch 1 Step 780 -- caption Loss: 2.051 retrieval Loss: 3.727 \n",
      "Current best loss!\n",
      "Epoch 1 Step 795 -- caption Loss: 2.051 retrieval Loss: 3.724 \n",
      "Current best loss!\n",
      "Epoch 1 Step 810 -- caption Loss: 2.050 retrieval Loss: 3.722 \n",
      "Current best loss!\n"
     ]
    }
   ],
   "source": [
    "logfile = open(\"log_6_cntd.txt\", \"w+\")\n",
    "\n",
    "model.train()\n",
    "best_losses = {mode:0x7FFFFFF for mode in MODES}\n",
    "for epoch in range(1, EPOCH+1):\n",
    "    losses = {mode:0.0 for mode in MODES}\n",
    "    batch_count = 0 \n",
    "    for step, (pixels, text) in enumerate(train_dataloader):\n",
    "        step_loss = {mode:None for mode in MODES}\n",
    "        for mode in MODES:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            output, i2t_embs, t2i_embs = model(pixels, text, mode=mode)\n",
    "\n",
    "            loss = output.loss\n",
    "            if mode == \"retrieval\":\n",
    "                logits_per_image = i2t_embs @ t2i_embs.t()\n",
    "                logits_per_text = logits_per_image.t()\n",
    "\n",
    "                caption_loss = contrastive_loss(logits_per_text)\n",
    "                image_loss = contrastive_loss(logits_per_image)\n",
    "                \n",
    "                loss += (caption_loss + image_loss) / 2.0\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for param in model.model.input_embeddings.parameters():\n",
    "                assert param.grad.shape[0] == len(tokenizer), \"Embedding and vocabulary sizes should be equal to each other\"\n",
    "                mask = torch.arange(param.grad.shape[0]) != ret_token_idx\n",
    "                param.grad[mask,:] = 0.0\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_count += 1\n",
    "            losses[mode] += loss.item()\n",
    "    \n",
    "        if step % 15 == 0:\n",
    "            is_best_loss = True\n",
    "            for mode in MODES:\n",
    "                total_loss_avg = losses[mode] / batch_count\n",
    "                if total_loss_avg > best_losses[mode]:\n",
    "                    is_best_loss = False\n",
    "\n",
    "            logfile.write(f\"Epoch {epoch} Step {step} -- \")\n",
    "            print(f\"Epoch {epoch} Step {step} -- \", end=\"\")\n",
    "            for m, l in losses.items():\n",
    "                logfile.write(f\"{m} Loss: {(l / batch_count):.3f} \")\n",
    "                print(f\"{m} Loss: {(l / batch_count):.3f}\", end=\" \")\n",
    "            print()\n",
    "            logfile.write(\"\\n\")\n",
    "            best_loss_avg = total_loss_avg\n",
    "            logfile.write(\"Current best loss!\\n\")\n",
    "            print(\"Current best loss!\")\n",
    "            logfile.flush()\n",
    "\n",
    "            if step % 105 == 0:\n",
    "                if is_best_loss:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': total_loss_avg\n",
    "                    }, \"best.ckpt\")\n",
    "\n",
    "logfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
