{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60aa97a-2c1d-4b1b-952a-934df8914a8f",
   "metadata": {},
   "source": [
    "###Â Todo\n",
    "\n",
    "I already had the COVID test for the BioVil-T ResNet model. Dig into it. Find that model, and use it as the feature extractor. During this process, I can also dig into other notebooks that I have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d5e1c2-16bd-46bf-a53f-202c08175cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/oince22/.conda/envs/fromage_scratch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Callable, Optional\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pydicom\n",
    "import os\n",
    "from torchvision.models import resnet50\n",
    "from glob import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc3cf7a-993b-4f8c-8b68-fb48202fd0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/users/oince22/hpc_run/fromage_scratch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7223625-5971-48ea-ab2e-0be9ff6e113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d3d78e-188c-4eeb-9472-da027b884f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e673d9f-e89e-4cca-a2f4-983526752c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "class BioViL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50()\n",
    "        self._initialize_resnet()\n",
    "        self.feature_extractor = self._get_feature_extractor()\n",
    "        \n",
    "    def _initialize_resnet(self):\n",
    "        base_keys = list(self.model.state_dict().keys())\n",
    "        model_state_dict = torch.load(\"biovil_backbone_2048.pt\")\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "        # torch.save(self.model.state_dict(), \"biovil_backbone_2048.pt\")\n",
    "\n",
    "    \n",
    "    def _get_feature_extractor(self):\n",
    "        self._return_nodes = {'avgpool': 'avgpool'}\n",
    "        return create_feature_extractor(self.model, return_nodes=self._return_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01f6a8a-859a-4595-b410-dd75984d8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioViL().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4968d5e-739e-4c07-b7bf-3a880d6c9ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25557032"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb00a01c-0d78-4a3a-af00-13858335d2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25557032"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6dc008-4d63-43a9-94db-f5cb3562079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.biovil_model = BioViL()\n",
    "        self.fc_dim = 2048\n",
    "        self.fc1 = torch.nn.Linear(self.fc_dim, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.biovil_model.requires_grad = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super(Model, self).train(mode=mode)\n",
    "        self.biovil_model.eval()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.biovil_model(x)['avgpool']\n",
    "        x = x.reshape(-1, self.fc_dim)\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bd8aad-6bc8-402d-96dd-4ebd956f4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandChannels:\n",
    "    def __call__(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        if data.shape[0] != 1:\n",
    "            raise ValueError(f\"Expected input of shape [1, H, W], found {data.shape}\")\n",
    "        return torch.repeat_interleave(data, 3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694a2cbc-40b2-4f21-ae3e-6e6cb09bf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, CenterCrop, RandomHorizontalFlip\n",
    "def create_chest_xray_transform_for_inference(resize: int, center_crop_size: int, train=False) -> Compose:\n",
    "    if not train:\n",
    "        transforms = [Resize(resize), CenterCrop(center_crop_size), ToTensor(), ExpandChannels()]\n",
    "    if train:\n",
    "        transforms = [Resize(resize), RandomHorizontalFlip(), CenterCrop(center_crop_size), ToTensor(), ExpandChannels()]\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3ecbea-679f-4463-9051-d9747a3f1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"chest_xray/chest_xray\"\n",
    "source_dirs = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "\n",
    "class ChestXRayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, split=\"train\"):\n",
    "        assert split in [\"train\", \"test\"]\n",
    "        \n",
    "        get_img_path = lambda img_fname, split, class_name: root_dir + \"/\" + split + \"/\" + class_name + \"/\" + img_fname\n",
    "        \n",
    "        def get_images(class_name):\n",
    "            images = [get_img_path(x, split, class_name) for x in os.listdir(root_dir + \"/\" + split + \"/\" + class_name) if x.lower().endswith('jpeg')]\n",
    "            print(f'Found {len(images)} {class_name} examples')\n",
    "            return images\n",
    "        \n",
    "        self.img_names = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.class_names = source_dirs\n",
    "        \n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            cur_img_paths = get_images(class_name)\n",
    "            self.img_names.extend(cur_img_paths)\n",
    "            self.labels.extend([label for _ in range(len(cur_img_paths))])\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_names[idx]\n",
    "        image = Image.open(image_path)\n",
    "        return self.transform(image), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08dd8f4f-f8e1-4d41-9ef8-0667f8f27a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_center_crop_size = 512\n",
    "TRANSFORM_RESIZE = 480\n",
    "\n",
    "train_transform = create_chest_xray_transform_for_inference(resize=TRANSFORM_RESIZE, center_crop_size=transform_center_crop_size, train=True)\n",
    "test_transform = create_chest_xray_transform_for_inference(resize=TRANSFORM_RESIZE, center_crop_size=transform_center_crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6f7d9b-aca7-405a-a080-d8229eb063cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1341 NORMAL examples\n",
      "Found 3875 PNEUMONIA examples\n",
      "Found 234 NORMAL examples\n",
      "Found 390 PNEUMONIA examples\n",
      "Length of train set   :   5216\n",
      "Length of test set    :   624\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChestXRayDataset(train_transform, split=\"train\")\n",
    "test_dataset = ChestXRayDataset(test_transform, split=\"test\")\n",
    "\n",
    "print(\"Length of train set   :  \", len(train_dataset))\n",
    "print(\"Length of test set    :  \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2990b52a-f65b-4760-82b6-2caf20c27324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training batches 290\n",
      "Lentgth of test batches 35\n"
     ]
    }
   ],
   "source": [
    "batch_size = 18\n",
    "\n",
    "data_train_len = torch.utils.data.DataLoader(train_dataset, batch_size= batch_size, shuffle=True)\n",
    "data_test_len = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Length of training batches\", len(data_train_len))\n",
    "print(\"Lentgth of test batches\", len(data_test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e089ae2-3dad-496e-99c7-3d5048d18e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dicom_to_jpg(input_path, output_path, resize=False, new_width=512):\n",
    "    # Load DICOM file\n",
    "    ds = pydicom.dcmread(input_path)\n",
    "\n",
    "    # Extract pixel data and normalize to range [0, 255]\n",
    "    pixel_data = ds.pixel_array\n",
    "    pixel_data = pixel_data.astype(np.float32)\n",
    "    pixel_data -= np.min(pixel_data)\n",
    "    pixel_data /= np.max(pixel_data)\n",
    "    pixel_data *= 255.0\n",
    "    pixel_data = np.uint8(pixel_data)\n",
    "\n",
    "    # Check PhotometricInterpretation for inversion\n",
    "    if ds.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        # Invert pixel values\n",
    "        pixel_data = 255 - pixel_data\n",
    "\n",
    "    # Histogram equalization\n",
    "    pixel_data = cv2.equalizeHist(pixel_data)\n",
    "\n",
    "    if resize:\n",
    "        height, width = pixel_data.shape\n",
    "\n",
    "        scale = new_width / width\n",
    "        new_height = int(height * scale)\n",
    "\n",
    "        pixel_data = cv2.resize(pixel_data, (new_width, new_height))\n",
    "\n",
    "    # Convert to JPEG with quality factor 95\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]\n",
    "    _, jpeg_data = cv2.imencode('.jpg', pixel_data, encode_param)\n",
    "\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(jpeg_data)\n",
    "\n",
    "    return pixel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c688318-2413-4878-b823-49117240f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15664ae-7b9b-4ab2-b23e-c745cbc57ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4a9798-1996-4f8f-8c69-8d19526796bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    print('Starting training..')\n",
    "    for e in range(0, epochs):\n",
    "        print('='*20)\n",
    "        print(f'Starting epoch {e + 1}/{epochs}')\n",
    "        print('='*20)\n",
    "        accuracy = 0\n",
    "\n",
    "        train_loss = 0.\n",
    "        val_loss = 0.\n",
    "\n",
    "        model.train() # set model to training phase\n",
    "        model.biovil_model.eval()\n",
    "        \n",
    "\n",
    "        for train_step, (images, labels) in enumerate(data_test_len):\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print('Evaluating at step', train_step)\n",
    "\n",
    "        model.eval() # set model to eval phase\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_step, (images, labels) in enumerate(data_test_len):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                outputs = outputs.detach().cpu()\n",
    "                labels = labels.detach().cpu()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                accuracy += sum((preds == labels).numpy())\n",
    "                print(f\"Val step: {val_step}\", end=\"\\r\")\n",
    "\n",
    "        val_loss /= (val_step + 1)\n",
    "        accuracy = accuracy/len(test_dataset)        \n",
    "        \n",
    "        model.train()\n",
    "        model.biovil_model.eval()\n",
    "\n",
    "        train_loss /= (train_step + 1)\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "    print('Training complete..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e4804c-0791-4284-8be9-8540b3a8f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predicts():\n",
    "    model.eval()\n",
    "    images, labels = next(iter(data_test_len))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    outputs = outputs.detach().cpu()\n",
    "    labels = labels.detach().cpu()\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    images = images.detach().cpu()\n",
    "    show_images(images, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4da4da95-49f6-4c27-a36c-4d52d92f2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/40\n",
      "====================\n",
      "Evaluating at step 34\n",
      "Training Loss: 0.7358\n",
      "Validation Loss: 0.6971, Accuracy: 0.3750\n",
      "====================\n",
      "Starting epoch 2/40\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87936ac0-25d3-4239-8f4e-d565d318247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2768c9b-97eb-4648-8769-641a606c309d",
   "metadata": {},
   "source": [
    "transform_center_crop_size = 480\n",
    "TRANSFORM_RESIZE = 512\n",
    "\n",
    "train_transform = create_chest_xray_transform_for_inference(resize=TRANSFORM_RESIZE, center_crop_size=transform_center_crop_size)\n",
    "test_transform = create_chest_xray_transform_for_inference(resize=TRANSFORM_RESIZE, center_crop_size=transform_center_crop_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de9df5-ea2a-4402-9b52-b66e9dc4b3ba",
   "metadata": {},
   "source": [
    "dicom_path = \"/datasets/mimic/physionet.org/files/mimic-cxr/2.0.0/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.dcm\"\n",
    "jpeg_path = \"test2_resized.jpg\"\n",
    "img = convert_dicom_to_jpg(dicom_path, jpeg_path, resize=True)\n",
    "print(img.shape)\n",
    "img = Image.fromarray(img)\n",
    "print(img)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.figure()\n",
    "img = test_transform(img)\n",
    "img = img.permute(1, 2, 0).numpy()\n",
    "img = img * 255\n",
    "img = img.astype(np.uint8)\n",
    "img = Image.fromarray(img)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
