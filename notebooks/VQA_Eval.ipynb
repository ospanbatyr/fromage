{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080f54cd-b31f-41d1-942b-1d98da46c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "sys.path.append('../fromage')\n",
    "from VQADataset import VQA_RADDataset\n",
    "from model import Fromage, FromageModel\n",
    "from experiment import Experiment\n",
    "from data import MIMICDataset, cxr_image_transform\n",
    "from utils import preprocess_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "498bf455-f4f9-442e-a9fe-b693037ed4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = cxr_image_transform(resize=512, center_crop_size=480, train=False) \n",
    "dataset = VQA_RADDataset(\"/kuacc/users/hpc-dtank/datasets/VQA_RAD\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bfcacef-05b2-4d95-b943-589c5b5b6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0667, 0.0667, 0.0667,  ..., 0.1882, 0.1725, 0.1412],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.2039, 0.1961, 0.2000],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.2235, 0.2157, 0.2235],\n",
       "          ...,\n",
       "          [0.0784, 0.0706, 0.0706,  ..., 0.0824, 0.0706, 0.0902],\n",
       "          [0.0784, 0.0784, 0.0784,  ..., 0.0824, 0.0706, 0.0902],\n",
       "          [0.0784, 0.0784, 0.0784,  ..., 0.0863, 0.0784, 0.0902]],\n",
       " \n",
       "         [[0.0667, 0.0667, 0.0667,  ..., 0.1882, 0.1725, 0.1412],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.2039, 0.1961, 0.2000],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.2235, 0.2157, 0.2235],\n",
       "          ...,\n",
       "          [0.0784, 0.0706, 0.0706,  ..., 0.0824, 0.0706, 0.0902],\n",
       "          [0.0784, 0.0784, 0.0784,  ..., 0.0824, 0.0706, 0.0902],\n",
       "          [0.0784, 0.0784, 0.0784,  ..., 0.0863, 0.0784, 0.0902]],\n",
       " \n",
       "         [[0.0667, 0.0667, 0.0667,  ..., 0.1882, 0.1725, 0.1412],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.2039, 0.1961, 0.2000],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.2235, 0.2157, 0.2235],\n",
       "          ...,\n",
       "          [0.0784, 0.0706, 0.0706,  ..., 0.0824, 0.0706, 0.0902],\n",
       "          [0.0784, 0.0784, 0.0784,  ..., 0.0824, 0.0706, 0.0902],\n",
       "          [0.0784, 0.0784, 0.0784,  ..., 0.0863, 0.0784, 0.0902]]]),\n",
       " 'Are the lungs normal appearing?',\n",
       " 'No')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0] # returns image, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb5d60bf-809b-4230-810e-becc96bb2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"../logs/checkpoints/untied_test/last.ckpt\"\n",
    "config_path = \"../config/train-untied.yaml\"\n",
    "dataset_path = \"../data/MIMIC_JPG.tsv\"\n",
    "img_path = \"/datasets/mimic/cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0/files\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open (config_path) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "model = Experiment(config)\n",
    "model = model.load_from_checkpoint(ckpt_path)\n",
    "model = model.model.to(device)\n",
    "model.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba751d9f-a501-4906-bf43-b0328e8c4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Question: Are the lungs normal appearing? Answer: \n",
      "Model Answer:   The lungs are normal, but there is a slight change in the shape of the lungs.\n",
      "\n",
      "Question: Are the lungs normal appearing? Answer:  The\n",
      "Correct Answer:  No\n"
     ]
    }
   ],
   "source": [
    "img, question, answer = dataset[0]\n",
    "prompt = str(\"Question: \" + question + \" Answer: \")\n",
    "print(\"Prompt: \", prompt)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    prompts = [img, prompt] \n",
    "    print(\"Model Answer: \", model.generate_for_images_and_texts(prompts, top_p=0.9, temperature=0.5))\n",
    "    \n",
    "print(\"Correct Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1981c3-38d9-424d-b2dd-1a87237c32f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fromage_env_kernel",
   "language": "python",
   "name": "fromage_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
